# NVIDIA RAG Blueprint (Optimized for 3x A100 40GB)

Use the following documentation to learn about this customized version of the NVIDIA RAG Blueprint.

- [Overview](#overview)
- [Key Features](#key-features)
- [Target Audience](#target-audience)
- [Software Components](#software-components)
- [Technical Diagram](#technical-diagram)
- [Minimum System Requirements](#minimum-system-requirements)
  - [OS Requirements](#os-requirements)
  - [Driver versions](#driver-versions)
  - [Hardware Requirements](#hardware-requirements)
- [Next Steps](#next-steps)
- [Inviting the community to contribute](#inviting-the-community-to-contribute)
- [License](#license)
- [Terms of Use](#terms-of-use)

## Overview

This repository is a customized adaptation of the **NVIDIA RAG Blueprint v2.1.0**.

While the original blueprint is designed for high-memory infrastructure (A100 80GB GPUs), **this version has been optimized to run fully functional on 3x NVIDIA A100 40GB GPUs**.

It serves as a reference solution for a foundational Retrieval Augmented Generation (RAG) pipeline, demonstrating how to set up a solution that uses NVIDIA NIM and GPU-accelerated components within more constrained hardware environments. To achieve this, specific models have been swapped for efficient, high-performance alternatives (such as the Nemotron Nano 4B), and the Vector Database has been offloaded to CPU to maximize GPU memory availability for inference and ingestion.

## Key Features

This customized blueprint includes the following active features and services:

- **Optimized Resource Usage:** configured to run on 40GB VRAM GPUs by using lightweight yet powerful NIMs.
- **Full Observability Stack:** Pre-configured and enabled by default, including:
  - **OpenTelemetry (OTel) Collector** for trace collection.
  - **Zipkin** for distributed tracing visualization.
  - **Prometheus** for metrics collection.
  - **Grafana** for monitoring dashboards.
- **Hybrid Search:** Enabled by default (Dense + Sparse search) for improved retrieval accuracy.
- **Multimodal Ingestion:** Support for extracting text, tables, charts, and infographics from PDFs, PPTX, and DOCX.
- **CPU-Offloaded Vector Database:** Milvus is configured to run on CPU to reserve GPU VRAM for LLM and Embedding tasks.
- **Sample User Interface:** Includes the `rag-playground` for immediate interaction.
- **OpenAI-compatible APIs**: Easy integration with existing tools.

## Target Audience

This blueprint is for:

- **Developers**: Developers who want a quick start to set up a RAG solution with NVIDIA NIM but have access to hardware with lower VRAM capacity (specifically A100 40GB cards) compared to the standard requirements.

## Software Components

The following are the components included in this customized blueprint, reflecting the active services:

* **NVIDIA NIM Microservices**
    * **Response Generation (Inference)**
      * [NIM of nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1](https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-4b-instruct) (Replaces the 49B model for efficiency).
    * **Retriever Models**
      * [NIM of nvidia/nv-embedqa-e5-v5](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5) (optimized with 1024 dimensions).
      * [NIM of nvidia/llama-3_2-nv-rerankqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2)
      * [NeMo Retriever Page Elements NIM](https://build.nvidia.com/nvidia/nemoretriever-page-elements-v2)
      * [NeMo Retriever Table Structure NIM](https://build.nvidia.com/nvidia/nemoretriever-table-structure-v1)
      * [NeMo Retriever Graphic Elements NIM](https://build.nvidia.com/nvidia/nemoretriever-graphic-elements-v1)
      * [PaddleOCR NIM](https://build.nvidia.com/baidu/paddleocr)

* **RAG Orchestrator server** - Langchain based.
* **Milvus Vector Database** - Running in **CPU-only mode** (v2.5.3) to conserve GPU memory.
* **Observability Services**:
    * Zipkin
    * Prometheus
    * Grafana
    * OpenTelemetry Collector
* **Ingestion** - [Nvidia-Ingest](https://github.com/NVIDIA/nv-ingest/tree/main) for parsing PDFs, Word, and PowerPoint documents.

## Technical Diagram

<p align="center">
<img src="./docs/arch_diagram.png" width="750">
</p>

The architecture remains logically consistent with the original blueprint. The workflow proceeds as follows:

1. **User Interaction**: Users interact via the **RAG Playground** or APIs.
2. **Query Processing**: The **RAG Server** processes the query.
3. **Retrieval**: The **Retriever** queries the **Milvus Vector Database** (running on CPU in this deployment) using embeddings generated by the **E5-v5 Embedding NIM**.
4. **Reranking**: Top results are refined by the **Reranking NIM**.
5. **Response Generation**: The context is sent to the **Llama 3.1 Nemotron Nano 4B NIM** to generate the answer.
6. **Observability**: Metrics and traces are collected in the background by OTel, Zipkin, and Prometheus.

## Minimum System Requirements

### OS Requirements
Ubuntu 22.04 OS

### Driver versions
- GPU Driver - 530.30.02 or later
- CUDA version - 12.6 or later

### Hardware Requirements
This specific deployment is tuned for:
- **GPUs**: 3x NVIDIA A100 (40GB VRAM)
- **Deployment Tool**: Docker & Docker Compose (v2.29.1 or later)

> **Note:** Unlike the original blueprint which requires A100 80GB cards, this version runs comfortably on 40GB cards due to the model swaps (Nano 4B & E5-v5) and CPU offloading of Milvus.

## Next Steps

- Do the procedures in [Get Started](./docs/quickstart.md) to deploy this blueprint
- See the [OpenAPI Specifications](./docs/api_reference)
- Explore notebooks that demonstrate how to use the APIs [here](./notebooks/)
- Explore [observability support](./docs/observability.md)
- Explore [best practices for enhancing accuracy or latency](./docs/accuracy_perf.md)
- Explore [migration guide](./docs/migration_guide.md) if you are migrating from rag v1.0.0 to this version.

## Inviting the community to contribute

We're posting these examples on GitHub to support the NVIDIA LLM community and facilitate feedback.
We invite contributions!
To open a GitHub issue or pull request, see the [contributing guidelines](./CONTRIBUTING.md).

## License

This project is a modification of the NVIDIA AI BLUEPRINT, licensed under the [Apache License, Version 2.0.](./LICENSE) This project will download and install additional third-party open source software projects and containers. Review [the license terms of these open source projects](./LICENSE-3rd-party.txt) before use.

Use of the models in this blueprint is governed by the [NVIDIA AI Foundation Models Community License](https://docs.nvidia.com/ai-foundation-models-community-license.pdf).

## Terms of Use
This blueprint is governed by the [NVIDIA Agreements | Enterprise Software | NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the [NVIDIA Agreements | Enterprise Software | Product Specific Terms for AI Product](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/). The models are governed by the [NVIDIA Agreements | Enterprise Software | NVIDIA Community Model License](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) and the [NVIDIA RAG dataset](https://github.com/NVIDIA-AI-Blueprints/rag/tree/v2.0.0/data/multimodal) which is governed by the [NVIDIA Asset License Agreement](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/data/LICENSE.DATA).